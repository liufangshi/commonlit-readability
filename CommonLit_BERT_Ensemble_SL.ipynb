{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import os\n",
    "import nltk\n",
    "import re\n",
    "# nltk.data.path.append(r\"C:\\Users\\liufa\\nltk_data\")\n",
    "\n",
    "# from nltk.tokenize import word_tokenize\n",
    "# nltk.download('brown')\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# BERT\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import one_hot, Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation, Dropout, Dense, Flatten, Lambda,\\\n",
    "    GlobalMaxPooling1D, LSTM, Bidirectional, concatenate, Embedding, multiply\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer, TFAutoModel, TFAutoModelForSequenceClassification, AdamW\n",
    "from transformers import TFBertModel, TFDistilBertModel, TFBertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url_legal</th>\n",
       "      <th>license</th>\n",
       "      <th>text</th>\n",
       "      <th>y</th>\n",
       "      <th>standard_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c12129c31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>When the young people returned to the ballroom...</td>\n",
       "      <td>-0.340259</td>\n",
       "      <td>0.464009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>85aa80a4c</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All through dinner time, Mrs. Fayre was somewh...</td>\n",
       "      <td>-0.315372</td>\n",
       "      <td>0.480805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b69ac6792</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>As Roger had predicted, the snow departed as q...</td>\n",
       "      <td>-0.580118</td>\n",
       "      <td>0.476676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dd1000b26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>And outside before the palace a great garden w...</td>\n",
       "      <td>-1.054013</td>\n",
       "      <td>0.450007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>37c1b32fb</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Once upon a time there were Three Bears who li...</td>\n",
       "      <td>0.247197</td>\n",
       "      <td>0.510845</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id url_legal license  \\\n",
       "0  c12129c31       NaN     NaN   \n",
       "1  85aa80a4c       NaN     NaN   \n",
       "2  b69ac6792       NaN     NaN   \n",
       "3  dd1000b26       NaN     NaN   \n",
       "4  37c1b32fb       NaN     NaN   \n",
       "\n",
       "                                                text         y  standard_error  \n",
       "0  When the young people returned to the ballroom... -0.340259        0.464009  \n",
       "1  All through dinner time, Mrs. Fayre was somewh... -0.315372        0.480805  \n",
       "2  As Roger had predicted, the snow departed as q... -0.580118        0.476676  \n",
       "3  And outside before the palace a great garden w... -1.054013        0.450007  \n",
       "4  Once upon a time there were Three Bears who li...  0.247197        0.510845  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# raw_data=pd.read_csv(r'../input/commonlitreadabilityprize/train.csv')\n",
    "raw_data=pd.read_csv(r'C:/Users/liufa/Desktop/Kaggle competition/train.csv')\n",
    "\n",
    "df = raw_data.rename(columns={'excerpt':'text','target':'y'})\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url_legal</th>\n",
       "      <th>license</th>\n",
       "      <th>text</th>\n",
       "      <th>y</th>\n",
       "      <th>standard_error</th>\n",
       "      <th>text_no_punct</th>\n",
       "      <th>text_tokenized</th>\n",
       "      <th>text_no_stopwords</th>\n",
       "      <th>text_lemmatizer</th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c12129c31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>When the young people returned to the ballroom...</td>\n",
       "      <td>-0.340259</td>\n",
       "      <td>0.464009</td>\n",
       "      <td>When the young people returned to the ballroom...</td>\n",
       "      <td>[when, the, young, people, returned, to, the, ...</td>\n",
       "      <td>[young, people, returned, ballroom, presented,...</td>\n",
       "      <td>[young, people, returned, ballroom, presented,...</td>\n",
       "      <td>young people returned ballroom presented decid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>85aa80a4c</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All through dinner time, Mrs. Fayre was somewh...</td>\n",
       "      <td>-0.315372</td>\n",
       "      <td>0.480805</td>\n",
       "      <td>All through dinner time Mrs Fayre was somewhat...</td>\n",
       "      <td>[all, through, dinner, time, mrs, fayre, was, ...</td>\n",
       "      <td>[dinner, time, mrs, fayre, somewhat, silent, e...</td>\n",
       "      <td>[dinner, time, mr, fayre, somewhat, silent, ey...</td>\n",
       "      <td>dinner time mr fayre somewhat silent eye resti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b69ac6792</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>As Roger had predicted, the snow departed as q...</td>\n",
       "      <td>-0.580118</td>\n",
       "      <td>0.476676</td>\n",
       "      <td>As Roger had predicted the snow departed as qu...</td>\n",
       "      <td>[as, roger, had, predicted, the, snow, departe...</td>\n",
       "      <td>[roger, predicted, snow, departed, quickly, ca...</td>\n",
       "      <td>[roger, predicted, snow, departed, quickly, ca...</td>\n",
       "      <td>roger predicted snow departed quickly came two...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dd1000b26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>And outside before the palace a great garden w...</td>\n",
       "      <td>-1.054013</td>\n",
       "      <td>0.450007</td>\n",
       "      <td>And outside before the palace a great garden w...</td>\n",
       "      <td>[and, outside, before, the, palace, a, great, ...</td>\n",
       "      <td>[outside, palace, great, garden, walled, round...</td>\n",
       "      <td>[outside, palace, great, garden, walled, round...</td>\n",
       "      <td>outside palace great garden walled round fille...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>37c1b32fb</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Once upon a time there were Three Bears who li...</td>\n",
       "      <td>0.247197</td>\n",
       "      <td>0.510845</td>\n",
       "      <td>Once upon a time there were Three Bears who li...</td>\n",
       "      <td>[once, upon, a, time, there, were, three, bear...</td>\n",
       "      <td>[upon, time, three, bears, lived, together, ho...</td>\n",
       "      <td>[upon, time, three, bear, lived, together, hou...</td>\n",
       "      <td>upon time three bear lived together house wood...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id url_legal license  \\\n",
       "0  c12129c31       NaN     NaN   \n",
       "1  85aa80a4c       NaN     NaN   \n",
       "2  b69ac6792       NaN     NaN   \n",
       "3  dd1000b26       NaN     NaN   \n",
       "4  37c1b32fb       NaN     NaN   \n",
       "\n",
       "                                                text         y  \\\n",
       "0  When the young people returned to the ballroom... -0.340259   \n",
       "1  All through dinner time, Mrs. Fayre was somewh... -0.315372   \n",
       "2  As Roger had predicted, the snow departed as q... -0.580118   \n",
       "3  And outside before the palace a great garden w... -1.054013   \n",
       "4  Once upon a time there were Three Bears who li...  0.247197   \n",
       "\n",
       "   standard_error                                      text_no_punct  \\\n",
       "0        0.464009  When the young people returned to the ballroom...   \n",
       "1        0.480805  All through dinner time Mrs Fayre was somewhat...   \n",
       "2        0.476676  As Roger had predicted the snow departed as qu...   \n",
       "3        0.450007  And outside before the palace a great garden w...   \n",
       "4        0.510845  Once upon a time there were Three Bears who li...   \n",
       "\n",
       "                                      text_tokenized  \\\n",
       "0  [when, the, young, people, returned, to, the, ...   \n",
       "1  [all, through, dinner, time, mrs, fayre, was, ...   \n",
       "2  [as, roger, had, predicted, the, snow, departe...   \n",
       "3  [and, outside, before, the, palace, a, great, ...   \n",
       "4  [once, upon, a, time, there, were, three, bear...   \n",
       "\n",
       "                                   text_no_stopwords  \\\n",
       "0  [young, people, returned, ballroom, presented,...   \n",
       "1  [dinner, time, mrs, fayre, somewhat, silent, e...   \n",
       "2  [roger, predicted, snow, departed, quickly, ca...   \n",
       "3  [outside, palace, great, garden, walled, round...   \n",
       "4  [upon, time, three, bears, lived, together, ho...   \n",
       "\n",
       "                                     text_lemmatizer  \\\n",
       "0  [young, people, returned, ballroom, presented,...   \n",
       "1  [dinner, time, mr, fayre, somewhat, silent, ey...   \n",
       "2  [roger, predicted, snow, departed, quickly, ca...   \n",
       "3  [outside, palace, great, garden, walled, round...   \n",
       "4  [upon, time, three, bear, lived, together, hou...   \n",
       "\n",
       "                                          text_clean  \n",
       "0  young people returned ballroom presented decid...  \n",
       "1  dinner time mr fayre somewhat silent eye resti...  \n",
       "2  roger predicted snow departed quickly came two...  \n",
       "3  outside palace great garden walled round fille...  \n",
       "4  upon time three bear lived together house wood...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "string.punctuation\n",
    "\n",
    "def remove_punct(text):\n",
    "    text_nopunct=''.join([char for char in text if char not in string.punctuation])\n",
    "    return text_nopunct\n",
    "\n",
    "df['text_no_punct']=df['text'].apply(lambda x: remove_punct(x))\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens=re.split('\\W+',text)\n",
    "    return tokens\n",
    "df['text_tokenized']=df['text_no_punct'].apply(lambda x: tokenize(x.lower()))\n",
    "\n",
    "stopwords=nltk.corpus.stopwords.words('english')\n",
    "def remove_stopwords(tokenized_list):\n",
    "    text=[word for word in tokenized_list if word not in stopwords]\n",
    "    return text\n",
    "df['text_no_stopwords']=df['text_tokenized'].apply(lambda x: remove_stopwords(x))\n",
    "\n",
    "wn=nltk.WordNetLemmatizer()\n",
    "def lemmatizing(tokenized_text):\n",
    "    text=[wn.lemmatize(word) for word in tokenized_text]\n",
    "    return text\n",
    "df['text_lemmatizer']=df['text_no_stopwords'].apply(lambda x: lemmatizing(x))\n",
    "\n",
    "df['text_clean']=df['text_lemmatizer'].apply(lambda x: ' '.join(x))\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url_legal</th>\n",
       "      <th>license</th>\n",
       "      <th>text</th>\n",
       "      <th>y</th>\n",
       "      <th>standard_error</th>\n",
       "      <th>text_no_punct</th>\n",
       "      <th>text_tokenized</th>\n",
       "      <th>text_no_stopwords</th>\n",
       "      <th>text_lemmatizer</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>num_char</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_sent</th>\n",
       "      <th>avg_word_len</th>\n",
       "      <th>avg_sent_len</th>\n",
       "      <th>num_cap_words</th>\n",
       "      <th>punct_ratio</th>\n",
       "      <th>stop_words_ratio</th>\n",
       "      <th>unique_words_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c12129c31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>When the young people returned to the ballroom...</td>\n",
       "      <td>-0.340259</td>\n",
       "      <td>0.464009</td>\n",
       "      <td>When the young people returned to the ballroom...</td>\n",
       "      <td>[when, the, young, people, returned, to, the, ...</td>\n",
       "      <td>[young, people, returned, ballroom, presented,...</td>\n",
       "      <td>[young, people, returned, ballroom, presented,...</td>\n",
       "      <td>young people returned ballroom presented decid...</td>\n",
       "      <td>819</td>\n",
       "      <td>179</td>\n",
       "      <td>11</td>\n",
       "      <td>4.575419</td>\n",
       "      <td>74.454545</td>\n",
       "      <td>0</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.452514</td>\n",
       "      <td>0.636872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>85aa80a4c</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All through dinner time, Mrs. Fayre was somewh...</td>\n",
       "      <td>-0.315372</td>\n",
       "      <td>0.480805</td>\n",
       "      <td>All through dinner time Mrs Fayre was somewhat...</td>\n",
       "      <td>[all, through, dinner, time, mrs, fayre, was, ...</td>\n",
       "      <td>[dinner, time, mrs, fayre, somewhat, silent, e...</td>\n",
       "      <td>[dinner, time, mr, fayre, somewhat, silent, ey...</td>\n",
       "      <td>dinner time mr fayre somewhat silent eye resti...</td>\n",
       "      <td>774</td>\n",
       "      <td>169</td>\n",
       "      <td>15</td>\n",
       "      <td>4.579882</td>\n",
       "      <td>51.600000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.396450</td>\n",
       "      <td>0.751479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b69ac6792</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>As Roger had predicted, the snow departed as q...</td>\n",
       "      <td>-0.580118</td>\n",
       "      <td>0.476676</td>\n",
       "      <td>As Roger had predicted the snow departed as qu...</td>\n",
       "      <td>[as, roger, had, predicted, the, snow, departe...</td>\n",
       "      <td>[roger, predicted, snow, departed, quickly, ca...</td>\n",
       "      <td>[roger, predicted, snow, departed, quickly, ca...</td>\n",
       "      <td>roger predicted snow departed quickly came two...</td>\n",
       "      <td>747</td>\n",
       "      <td>166</td>\n",
       "      <td>11</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>67.909091</td>\n",
       "      <td>2</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.451807</td>\n",
       "      <td>0.771084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dd1000b26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>And outside before the palace a great garden w...</td>\n",
       "      <td>-1.054013</td>\n",
       "      <td>0.450007</td>\n",
       "      <td>And outside before the palace a great garden w...</td>\n",
       "      <td>[and, outside, before, the, palace, a, great, ...</td>\n",
       "      <td>[outside, palace, great, garden, walled, round...</td>\n",
       "      <td>[outside, palace, great, garden, walled, round...</td>\n",
       "      <td>outside palace great garden walled round fille...</td>\n",
       "      <td>747</td>\n",
       "      <td>164</td>\n",
       "      <td>5</td>\n",
       "      <td>4.554878</td>\n",
       "      <td>149.400000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.414634</td>\n",
       "      <td>0.719512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>37c1b32fb</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Once upon a time there were Three Bears who li...</td>\n",
       "      <td>0.247197</td>\n",
       "      <td>0.510845</td>\n",
       "      <td>Once upon a time there were Three Bears who li...</td>\n",
       "      <td>[once, upon, a, time, there, were, three, bear...</td>\n",
       "      <td>[upon, time, three, bears, lived, together, ho...</td>\n",
       "      <td>[upon, time, three, bear, lived, together, hou...</td>\n",
       "      <td>upon time three bear lived together house wood...</td>\n",
       "      <td>577</td>\n",
       "      <td>147</td>\n",
       "      <td>5</td>\n",
       "      <td>3.925170</td>\n",
       "      <td>115.400000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.496599</td>\n",
       "      <td>0.346939</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id url_legal license  \\\n",
       "0  c12129c31       NaN     NaN   \n",
       "1  85aa80a4c       NaN     NaN   \n",
       "2  b69ac6792       NaN     NaN   \n",
       "3  dd1000b26       NaN     NaN   \n",
       "4  37c1b32fb       NaN     NaN   \n",
       "\n",
       "                                                text         y  \\\n",
       "0  When the young people returned to the ballroom... -0.340259   \n",
       "1  All through dinner time, Mrs. Fayre was somewh... -0.315372   \n",
       "2  As Roger had predicted, the snow departed as q... -0.580118   \n",
       "3  And outside before the palace a great garden w... -1.054013   \n",
       "4  Once upon a time there were Three Bears who li...  0.247197   \n",
       "\n",
       "   standard_error                                      text_no_punct  \\\n",
       "0        0.464009  When the young people returned to the ballroom...   \n",
       "1        0.480805  All through dinner time Mrs Fayre was somewhat...   \n",
       "2        0.476676  As Roger had predicted the snow departed as qu...   \n",
       "3        0.450007  And outside before the palace a great garden w...   \n",
       "4        0.510845  Once upon a time there were Three Bears who li...   \n",
       "\n",
       "                                      text_tokenized  \\\n",
       "0  [when, the, young, people, returned, to, the, ...   \n",
       "1  [all, through, dinner, time, mrs, fayre, was, ...   \n",
       "2  [as, roger, had, predicted, the, snow, departe...   \n",
       "3  [and, outside, before, the, palace, a, great, ...   \n",
       "4  [once, upon, a, time, there, were, three, bear...   \n",
       "\n",
       "                                   text_no_stopwords  \\\n",
       "0  [young, people, returned, ballroom, presented,...   \n",
       "1  [dinner, time, mrs, fayre, somewhat, silent, e...   \n",
       "2  [roger, predicted, snow, departed, quickly, ca...   \n",
       "3  [outside, palace, great, garden, walled, round...   \n",
       "4  [upon, time, three, bears, lived, together, ho...   \n",
       "\n",
       "                                     text_lemmatizer  \\\n",
       "0  [young, people, returned, ballroom, presented,...   \n",
       "1  [dinner, time, mr, fayre, somewhat, silent, ey...   \n",
       "2  [roger, predicted, snow, departed, quickly, ca...   \n",
       "3  [outside, palace, great, garden, walled, round...   \n",
       "4  [upon, time, three, bear, lived, together, hou...   \n",
       "\n",
       "                                          text_clean  num_char  num_words  \\\n",
       "0  young people returned ballroom presented decid...       819        179   \n",
       "1  dinner time mr fayre somewhat silent eye resti...       774        169   \n",
       "2  roger predicted snow departed quickly came two...       747        166   \n",
       "3  outside palace great garden walled round fille...       747        164   \n",
       "4  upon time three bear lived together house wood...       577        147   \n",
       "\n",
       "   num_sent  avg_word_len  avg_sent_len  num_cap_words  punct_ratio  \\\n",
       "0        11      4.575419     74.454545              0        0.033   \n",
       "1        15      4.579882     51.600000              6        0.072   \n",
       "2        11      4.500000     67.909091              2        0.063   \n",
       "3         5      4.554878    149.400000              0        0.044   \n",
       "4         5      3.925170    115.400000              0        0.055   \n",
       "\n",
       "   stop_words_ratio  unique_words_ratio  \n",
       "0          0.452514            0.636872  \n",
       "1          0.396450            0.751479  \n",
       "2          0.451807            0.771084  \n",
       "3          0.414634            0.719512  \n",
       "4          0.496599            0.346939  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_punct(text):\n",
    "    ct=sum([1 for char in text if char in string.punctuation])\n",
    "    return round(ct/(len(text)-text.count(' ')),3)\n",
    "\n",
    "def count_capital_words(text):\n",
    "    return sum(map(str.isupper,text.split()))\n",
    "    \n",
    "def count_sent(text):\n",
    "    return len(nltk.sent_tokenize(text))\n",
    "\n",
    "def count_stopwords(text):\n",
    "    stopwords=nltk.corpus.stopwords.words('english')\n",
    "    word_tokens = nltk.word_tokenize(text)\n",
    "    n_stopwords = [w for w in word_tokens if w in stopwords]\n",
    "    return len(n_stopwords)\n",
    "\n",
    "def count_unique_words(text):\n",
    "    return len(set(text.split()))\n",
    "\n",
    "df['num_char']=df['text'].apply(lambda x: len(x)-x.count(' '))\n",
    "df['num_words']=df['text_tokenized'].apply(lambda x: len(x))\n",
    "df['num_sent']=df['text'].apply(lambda x: count_sent(x))\n",
    "df['avg_word_len'] = df['num_char']/df['num_words']\n",
    "df['avg_sent_len'] = df['num_char']/df['num_sent']\n",
    "df['num_cap_words'] = df['text'].apply(lambda x: count_capital_words(x))\n",
    "df['punct_ratio']=df['text'].apply(lambda x:count_punct(x))\n",
    "df['stop_words_ratio'] = df['text'].apply(lambda x: count_stopwords(x))/df['num_words']\n",
    "df['unique_words_ratio'] = df['text'].apply(lambda x: count_unique_words(x))/df['num_words']\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Topic modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.98142114,  0.22000603, -0.76107608, ..., -0.66349325,\n",
       "        -0.55668465,  1.36510705],\n",
       "       [-1.38668274, -1.49922092, -0.76107608, ..., -0.97586316,\n",
       "        -0.41923363,  1.53478439],\n",
       "       [-1.16527466, -1.32136986, -0.31725371, ..., -0.03875343,\n",
       "        -0.21277577,  1.24941573],\n",
       "       ...,\n",
       "       [ 1.37610512,  0.99069397,  0.12656867, ..., -0.35112334,\n",
       "        -0.40765153,  0.6990866 ],\n",
       "       [ 0.33644976, -0.37283085, -0.98298727, ...,  0.42980143,\n",
       "        -0.21205825,  1.05782364],\n",
       "       [-1.31929767, -1.20280248,  4.34288121, ...,  6.2867372 ,\n",
       "        -1.97179774, -5.12382892]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train test split\n",
    "cols = [c for c in df.columns if c not in ['id',\n",
    "                                           'url_legal',\n",
    "                                           'license',\n",
    "                                           'standard_error',\n",
    "                                           'text_clean',\n",
    "                                           'text_no_punct',\n",
    "                                           'text_tokenized',\n",
    "                                           'text_no_stopwords',\n",
    "                                           'text_lemmatizer',\n",
    "                                           'y']]\n",
    "X = df[cols]\n",
    "y = df[['y']]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train_num = X_train[[c for c in X.columns if c != 'text']].to_numpy()\n",
    "X_test_num = X_test[[c for c in X.columns if c != 'text']].to_numpy()\n",
    "\n",
    "y_train = y_train.values\n",
    "y_test = y_test.values\n",
    "\n",
    "\n",
    "# standard scaler\n",
    "scaler = StandardScaler().fit(X_train_num)\n",
    "X_train_num = scaler.transform(X_train_num)\n",
    "X_test_num = scaler.transform(X_test_num)\n",
    "\n",
    "X_train_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 x 2267 x 300\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': array([[    0,   133,   745, ...,     1,     1,     1],\n",
       "        [    0,   133,  1114, ...,     1,     1,     1],\n",
       "        [    0, 37818,     5, ...,     1,     1,     1],\n",
       "        ...,\n",
       "        [    0,   100,  1017, ...,     1,     1,     1],\n",
       "        [    0,  1121,   209, ...,     1,     1,     1],\n",
       "        [    0, 39488,    16, ...,     1,     1,     1]]),\n",
       " 'attention_mask': array([[1, 1, 1, ..., 0, 0, 0],\n",
       "        [1, 1, 1, ..., 0, 0, 0],\n",
       "        [1, 1, 1, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1, ..., 0, 0, 0],\n",
       "        [1, 1, 1, ..., 0, 0, 0],\n",
       "        [1, 1, 1, ..., 0, 0, 0]])}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BERT tokenizer\n",
    "# 'bert-base-cased', 'distilbert-base-cased', 'roberta-base', 'distilroberta-base'\n",
    "model_path = 'roberta-base'\n",
    "max_len = 300\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "def tokenize_data(X):\n",
    "    tokenized = tokenizer(X.tolist(), padding='max_length',truncation=True, max_length=max_len, return_tensors=\"np\") \n",
    "    return {feat: tokenized[feat] for feat in tokenizer.model_input_names}\n",
    "    \n",
    "X_train_tokenized = tokenize_data(X_train['text'])\n",
    "X_test_tokenized = tokenize_data(X_test['text'])\n",
    "\n",
    "print (\n",
    "str(len(X_train_tokenized))\\\n",
    "+ ' x ' +\\\n",
    "str(len(X_train_tokenized['input_ids']))\\\n",
    "+ ' x ' +\\\n",
    "str(len(X_train_tokenized['input_ids'][0]))\n",
    ")\n",
    "\n",
    "X_train_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.51834981],\n",
       "       [-0.54880659],\n",
       "       [-0.19326203],\n",
       "       ...,\n",
       "       [-1.58438395],\n",
       "       [-2.03468808],\n",
       "       [-0.6052157 ]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help functions\n",
    "\n",
    "# to define 'rmse' as loss instead of 'mse'\n",
    "def rmse(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true))) \n",
    "\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss', \n",
    "    factor = 0.5 , \n",
    "    patience=1, \n",
    "    mode='min', \n",
    "    verbose=1)\n",
    "\n",
    "early_stop = keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=3,\n",
    "    verbose=1,\n",
    "    restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Base BERT embedding + TF-IDF + numerical features\n",
    "\n",
    "# BERTmodel = TFBertModel.from_pretrained(model_path)\n",
    "\n",
    "# train_sent_embedding = []\n",
    "# for x in a:\n",
    "#     train_sent_embedding.append(BERTmodel(x).last_hidden_state[0][0])\n",
    "    \n",
    "\n",
    "# df_final = train_final.join(train_sent_embedding)\n",
    "# df_final.head()\n",
    "\n",
    "# # MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x000002552F9AF908>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x000002552F9AF908>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py:5049: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 300)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 300)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_roberta_model (TFRobertaMode TFBaseModelOutputWit 124645632   input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 300, 512)     393728      tf_roberta_model[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 300, 1)       513         dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "multiply (Multiply)             (None, 300, 768)     0           dense_1[0][0]                    \n",
      "                                                                 tf_roberta_model[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(None, 9)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, 768)          0           multiply[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 9)            90          input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 270)          207630      lambda[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_37 (Dropout)            (None, 9)            0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 279)          0           dense_2[0][0]                    \n",
      "                                                                 dropout_37[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 1)            280         concatenate[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 125,247,873\n",
      "Trainable params: 125,247,873\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# # Ensemble model architecture BERT classifier + numerical features\n",
    "def softMaxAxis1(x):\n",
    "    return tf.keras.activations.softmax(x,axis=1)\n",
    "\n",
    "input_ids = tf.keras.Input(shape=(max_len, ),dtype='int32')\n",
    "attention_mask = tf.keras.Input(shape=(max_len, ),dtype='int32')\n",
    "\n",
    "feat_num = X_train_num.shape[1]\n",
    "numeric_input = tf.keras.Input(shape=(feat_num,),dtype='float32')\n",
    "\n",
    "# change huggingface model if needed\n",
    "bert_layer = TFAutoModel.from_pretrained(model_path)(input_ids, attention_mask)[0]\n",
    "\n",
    "# cls_token = bert_layer[:, 0, :]\n",
    "\n",
    "# attention layer\n",
    "weight_layer = Dense(512, activation='tanh')(bert_layer)\n",
    "weight_layer = Dense(1, activation=softMaxAxis1)(weight_layer)\n",
    "layer = multiply([weight_layer,bert_layer])\n",
    "layer = Lambda(lambda x: K.sum(x, axis=1))(layer)\n",
    "layer = Dense(270, activation='relu')(layer)\n",
    "\n",
    "num_layer = Dense(9, activation='relu')(numeric_input)\n",
    "num_layer = Dropout(0.3)(num_layer)\n",
    "\n",
    "layer = concatenate([layer, num_layer])\n",
    "# layer = Dense(180, activation='relu')(layer)\n",
    "# layer = Dropout(0.2)(layer)\n",
    "out = Dense(1, activation='linear')(layer)\n",
    "\n",
    "model = tf.keras.Model(inputs=[input_ids , attention_mask, numeric_input], outputs=out)\n",
    "\n",
    "# freezing BERT embeddings\n",
    "# model.layers[2].trainable = False\n",
    "\n",
    "model.compile(optimizer=Adam(2e-5), loss=rmse) # small training rates are necessary!\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "491/491 [==============================] - ETA: 0s - loss: 0.6607WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "491/491 [==============================] - 114s 219ms/step - loss: 0.6607 - val_loss: 0.5829\n",
      "Epoch 2/15\n",
      "491/491 [==============================] - 106s 215ms/step - loss: 0.5202 - val_loss: 0.5700\n",
      "Epoch 3/15\n",
      "491/491 [==============================] - 108s 220ms/step - loss: 0.4157 - val_loss: 0.5104\n",
      "Epoch 4/15\n",
      "491/491 [==============================] - 110s 223ms/step - loss: 0.3528 - val_loss: 0.5457\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "Epoch 5/15\n",
      "491/491 [==============================] - 104s 211ms/step - loss: 0.2731 - val_loss: 0.5564\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-06.\n",
      "Epoch 6/15\n",
      "491/491 [==============================] - 104s 211ms/step - loss: 0.2110 - val_loss: 0.5220\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00006: early stopping\n",
      "18/18 [==============================] - 7s 386ms/step - loss: 0.5423\n",
      "Test Scores: 0.5422965288162231\n"
     ]
    }
   ],
   "source": [
    "history = model.fit([X_train_tokenized['input_ids'],\\\n",
    "                     X_train_tokenized['attention_mask'],\\\n",
    "                     X_train_num],\\\n",
    "                    y_train, epochs=15, verbose=1, batch_size=3, validation_split=0.35, callbacks=[reduce_lr,early_stop])\n",
    "\n",
    "score = model.evaluate([X_test_tokenized['input_ids'],\\\n",
    "                        X_test_tokenized['attention_mask'],\\\n",
    "                        X_test_num],\\\n",
    "                       y_test, verbose=1)\n",
    "print(\"Test Scores:\", score)\n",
    "\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.layers[2].output\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kerastuner.tuners import BayesianOptimization\n",
    "def build_model(hp):\n",
    "    input_ids = tf.keras.Input(shape=(max_len, ),dtype='int32')\n",
    "    attention_mask = tf.keras.Input(shape=(max_len, ),dtype='int32')\n",
    "\n",
    "    feat_num = X_train_num.shape[1]\n",
    "    numeric_input = tf.keras.Input(shape=(feat_num,),dtype='float32')\n",
    "\n",
    "    # change huggingface model if needed\n",
    "    bert_layer = TFAutoModel.from_pretrained(model_path)(input_ids, attention_mask)[0]\n",
    "\n",
    "    layer = Flatten()(bert_layer)\n",
    "    layer = Dense(769, activation='relu')(layer)\n",
    "    layer = Concatenate(axis=1)([layer, numeric_input])\n",
    "    layer = Dense(hp.Int('dense_units1',min_value=40,max_value=300,step=10), activation='relu')(layer)\n",
    "    layer = Dropout(0.2)(layer)\n",
    "    out = Dense(1, activation='linear')(layer)\n",
    "\n",
    "    model = tf.keras.Model(inputs=[input_ids , attention_mask, numeric_input], outputs=out)\n",
    "\n",
    "#     model.compile(optimizer=Adam(2e-5), loss='mse', metrics=['mean_squared_error']) \n",
    "    model.compile(optimizer=Adam(2e-5), loss='mse', metrics=[tf.keras.metrics.RootMeanSquaredError()]) \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "bayesian_opt_tuner = BayesianOptimization(\n",
    "    build_model,\n",
    "    objective='val_mean_squared_error',\n",
    "    max_trials=10,\n",
    "    executions_per_trial=1,\n",
    "    directory=os.path.normpath(r'C:\\Users\\liufa\\keras_tuning'),\n",
    "    project_name='kerastuner_bayesian_poc',\n",
    "    overwrite=True)\n",
    "\n",
    "bayesian_opt_tuner.search([X_train_tokenized['input_ids'],X_train_tokenized['attention_mask'],X_train_num],\\\n",
    "                           y_train, epochs=15, batch_size=2, callbacks=[early_stop], validation_split=0.3, verbose=1)\n",
    "     #validation_data=(X_test, y_test)\n",
    "#     callbacks=[early_stop], \n",
    "\n",
    "bayes_opt_model_best_model = bayesian_opt_tuner.get_best_models(num_models=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayes_opt_model_best_model[0].summary()\n",
    "best_hp = bayesian_opt_tuner.get_best_hyperparameters(1)[0]\n",
    "print(best_hp.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################ Submission ##########################################\n",
    "\n",
    "df_test = pd.read_csv(r'../input/commonlitreadabilityprize/test.csv')\n",
    "\n",
    "df_test = df_test.rename(columns={'excerpt':'text','target':'y'})\n",
    "df_test['text_no_punct']=df_test['text'].apply(lambda x: remove_punct(x))\n",
    "df_test['text_tokenized']=df_test['text_no_punct'].apply(lambda x: tokenize(x.lower()))\n",
    "stopwords=nltk.corpus.stopwords.words('english')\n",
    "df_test['text_no_stopwords']=df_test['text_tokenized'].apply(lambda x: remove_stopwords(x))\n",
    "wn=nltk.WordNetLemmatizer()\n",
    "df_test['text_lemmatizer']=df_test['text_no_stopwords'].apply(lambda x: lemmatizing(x))\n",
    "df_test['text_clean']=df_test['text_lemmatizer'].apply(lambda x: ' '.join(x))\n",
    "df_test['sentence_len']=df_test['text'].apply(lambda x: len(x)-x.count(' '))\n",
    "df_test['word_len']=df_test['text_tokenized'].apply(lambda x: len(x))\n",
    "df_test['punct_count']=df_test['text'].apply(lambda x:count_punct(x))\n",
    "df_test['d2v_vector'] = df_test['text_lemmatizer'].apply(lambda x:d2v_model.infer_vector(x))\n",
    "\n",
    "X_test_LSTM = df_test['text_clean']\n",
    "X_test_LSTM = tokenizer.texts_to_sequences(X_test_LSTM)\n",
    "X_test_LSTM = pad_sequences(X_test_LSTM, padding='post', maxlen=maxlen)\n",
    "df_test['lstm_feature0'] = model.predict(X_test_LSTM)\n",
    "\n",
    "\n",
    "test_d2v_features = pd.DataFrame(df_test[\"d2v_vector\"].to_list(), columns=['d2v_feature'+str(i) for i in range(40)])\n",
    "X_test_ridge = df_test.drop(columns = ['id','url_legal','license','text','y','standard_error',\n",
    " 'text_no_punct','text_tokenized','text_no_stopwords','text_lemmatizer','text_clean','d2v_vector'\n",
    "                        ])\n",
    "\n",
    "test_final = df_test.join(test_d2v_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['target'] = ridge.predict(X_test_ridge)\n",
    "output = df_test[['id','target']]\n",
    "output.to_csv('./submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
