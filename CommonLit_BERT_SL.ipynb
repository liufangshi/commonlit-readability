{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import os\n",
    "import nltk\n",
    "# nltk.data.path.append(r\"C:\\Users\\liufa\\nltk_data\")\n",
    "\n",
    "# from nltk.tokenize import word_tokenize\n",
    "# nltk.download('brown')\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "# BERT\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import one_hot, Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation, Dropout, Dense, Flatten, Lambda,\\\n",
    "    GlobalMaxPooling1D, LSTM, Bidirectional, concatenate, Embedding, multiply\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "from transformers import AutoTokenizer, TFAutoModel, TFAutoModelForSequenceClassification, AdamW\n",
    "from transformers import TFBertModel, TFDistilBertModel, TFBertForSequenceClassification\n",
    "\n",
    "import gc\n",
    "gc.enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url_legal</th>\n",
       "      <th>license</th>\n",
       "      <th>text</th>\n",
       "      <th>y</th>\n",
       "      <th>standard_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c12129c31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>When the young people returned to the ballroom...</td>\n",
       "      <td>-0.340259</td>\n",
       "      <td>0.464009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>85aa80a4c</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All through dinner time, Mrs. Fayre was somewh...</td>\n",
       "      <td>-0.315372</td>\n",
       "      <td>0.480805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b69ac6792</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>As Roger had predicted, the snow departed as q...</td>\n",
       "      <td>-0.580118</td>\n",
       "      <td>0.476676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dd1000b26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>And outside before the palace a great garden w...</td>\n",
       "      <td>-1.054013</td>\n",
       "      <td>0.450007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>37c1b32fb</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Once upon a time there were Three Bears who li...</td>\n",
       "      <td>0.247197</td>\n",
       "      <td>0.510845</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id url_legal license  \\\n",
       "0  c12129c31       NaN     NaN   \n",
       "1  85aa80a4c       NaN     NaN   \n",
       "2  b69ac6792       NaN     NaN   \n",
       "3  dd1000b26       NaN     NaN   \n",
       "4  37c1b32fb       NaN     NaN   \n",
       "\n",
       "                                                text         y  standard_error  \n",
       "0  When the young people returned to the ballroom... -0.340259        0.464009  \n",
       "1  All through dinner time, Mrs. Fayre was somewh... -0.315372        0.480805  \n",
       "2  As Roger had predicted, the snow departed as q... -0.580118        0.476676  \n",
       "3  And outside before the palace a great garden w... -1.054013        0.450007  \n",
       "4  Once upon a time there were Three Bears who li...  0.247197        0.510845  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data=pd.read_csv(r'C:\\Users\\liufa\\Desktop\\Kaggle competition\\train.csv')\n",
    "\n",
    "df = raw_data.rename(columns={'excerpt':'text','target':'y'})\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "string.punctuation\n",
    "\n",
    "def remove_punct(text):\n",
    "    text_nopunct=''.join([char for char in text if char not in string.punctuation])\n",
    "    return text_nopunct\n",
    "\n",
    "df['text_no_punct']=df['text'].apply(lambda x: remove_punct(x))\n",
    "\n",
    "import re\n",
    "def tokenize(text):\n",
    "    tokens=re.split('\\W+',text)\n",
    "    return tokens\n",
    "df['text_tokenized']=df['text_no_punct'].apply(lambda x: tokenize(x.lower()))\n",
    "\n",
    "# import nltk\n",
    "# stopwords=nltk.corpus.stopwords.words('english')\n",
    "# def remove_stopwords(tokenized_list):\n",
    "#     text=[word for word in tokenized_list if word not in stopwords]\n",
    "#     return text\n",
    "# df['text_no_stopwords']=df['text_tokenized'].apply(lambda x: remove_stopwords(x))\n",
    "\n",
    "# wn=nltk.WordNetLemmatizer()\n",
    "# def lemmatizing(tokenized_text):\n",
    "#     text=[wn.lemmatize(word) for word in tokenized_text]\n",
    "#     return text\n",
    "# df['text_lemmatizer']=df['text_no_stopwords'].apply(lambda x: lemmatizing(x))\n",
    "\n",
    "# df['text_clean']=df['text_lemmatizer'].apply(lambda x: ' '.join(x))\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url_legal</th>\n",
       "      <th>license</th>\n",
       "      <th>text</th>\n",
       "      <th>y</th>\n",
       "      <th>standard_error</th>\n",
       "      <th>text_no_punct</th>\n",
       "      <th>text_tokenized</th>\n",
       "      <th>sentence_len</th>\n",
       "      <th>word_len</th>\n",
       "      <th>punct_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c12129c31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>When the young people returned to the ballroom...</td>\n",
       "      <td>-0.340259</td>\n",
       "      <td>0.464009</td>\n",
       "      <td>When the young people returned to the ballroom...</td>\n",
       "      <td>[when, the, young, people, returned, to, the, ...</td>\n",
       "      <td>819</td>\n",
       "      <td>179</td>\n",
       "      <td>0.033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>85aa80a4c</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All through dinner time, Mrs. Fayre was somewh...</td>\n",
       "      <td>-0.315372</td>\n",
       "      <td>0.480805</td>\n",
       "      <td>All through dinner time Mrs Fayre was somewhat...</td>\n",
       "      <td>[all, through, dinner, time, mrs, fayre, was, ...</td>\n",
       "      <td>774</td>\n",
       "      <td>169</td>\n",
       "      <td>0.072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b69ac6792</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>As Roger had predicted, the snow departed as q...</td>\n",
       "      <td>-0.580118</td>\n",
       "      <td>0.476676</td>\n",
       "      <td>As Roger had predicted the snow departed as qu...</td>\n",
       "      <td>[as, roger, had, predicted, the, snow, departe...</td>\n",
       "      <td>747</td>\n",
       "      <td>166</td>\n",
       "      <td>0.063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dd1000b26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>And outside before the palace a great garden w...</td>\n",
       "      <td>-1.054013</td>\n",
       "      <td>0.450007</td>\n",
       "      <td>And outside before the palace a great garden w...</td>\n",
       "      <td>[and, outside, before, the, palace, a, great, ...</td>\n",
       "      <td>747</td>\n",
       "      <td>164</td>\n",
       "      <td>0.044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>37c1b32fb</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Once upon a time there were Three Bears who li...</td>\n",
       "      <td>0.247197</td>\n",
       "      <td>0.510845</td>\n",
       "      <td>Once upon a time there were Three Bears who li...</td>\n",
       "      <td>[once, upon, a, time, there, were, three, bear...</td>\n",
       "      <td>577</td>\n",
       "      <td>147</td>\n",
       "      <td>0.055</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id url_legal license  \\\n",
       "0  c12129c31       NaN     NaN   \n",
       "1  85aa80a4c       NaN     NaN   \n",
       "2  b69ac6792       NaN     NaN   \n",
       "3  dd1000b26       NaN     NaN   \n",
       "4  37c1b32fb       NaN     NaN   \n",
       "\n",
       "                                                text         y  \\\n",
       "0  When the young people returned to the ballroom... -0.340259   \n",
       "1  All through dinner time, Mrs. Fayre was somewh... -0.315372   \n",
       "2  As Roger had predicted, the snow departed as q... -0.580118   \n",
       "3  And outside before the palace a great garden w... -1.054013   \n",
       "4  Once upon a time there were Three Bears who li...  0.247197   \n",
       "\n",
       "   standard_error                                      text_no_punct  \\\n",
       "0        0.464009  When the young people returned to the ballroom...   \n",
       "1        0.480805  All through dinner time Mrs Fayre was somewhat...   \n",
       "2        0.476676  As Roger had predicted the snow departed as qu...   \n",
       "3        0.450007  And outside before the palace a great garden w...   \n",
       "4        0.510845  Once upon a time there were Three Bears who li...   \n",
       "\n",
       "                                      text_tokenized  sentence_len  word_len  \\\n",
       "0  [when, the, young, people, returned, to, the, ...           819       179   \n",
       "1  [all, through, dinner, time, mrs, fayre, was, ...           774       169   \n",
       "2  [as, roger, had, predicted, the, snow, departe...           747       166   \n",
       "3  [and, outside, before, the, palace, a, great, ...           747       164   \n",
       "4  [once, upon, a, time, there, were, three, bear...           577       147   \n",
       "\n",
       "   punct_count  \n",
       "0        0.033  \n",
       "1        0.072  \n",
       "2        0.063  \n",
       "3        0.044  \n",
       "4        0.055  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sentence_len']=df['text'].apply(lambda x: len(x)-x.count(' '))\n",
    "df['word_len']=df['text_tokenized'].apply(lambda x: len(x))\n",
    "def count_punct(text):\n",
    "    ct=sum([1 for char in text if char in string.punctuation])\n",
    "    return round(ct/(len(text)-text.count(' ')),3)\n",
    "df['punct_count']=df['text'].apply(lambda x:count_punct(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'url_legal', 'license', 'text', 'y', 'standard_error',\n",
       "       'text_no_punct', 'text_tokenized', 'sentence_len', 'word_len',\n",
       "       'punct_count'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help functions\n",
    "\n",
    "# to define 'rmse' as loss instead of 'mse'\n",
    "def rmse(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true))) \n",
    "\n",
    "def softMaxAxis1(x):\n",
    "    return tf.keras.activations.softmax(x,axis=1)\n",
    "\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss', \n",
    "    factor = 0.5 , \n",
    "    patience=1, \n",
    "    mode='min', \n",
    "    verbose=1)\n",
    "\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=2,\n",
    "    verbose=1,\n",
    "    restore_best_weights=True)\n",
    "\n",
    "# checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "#     \"model_fold_{}\".format(fold) +\".h5\", \n",
    "#     monitor='val_loss', \n",
    "#     save_best_only=True, \n",
    "#     mode='min', \n",
    "#     verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT tokenizer\n",
    "# 'bert-base-cased', 'distilbert-base-cased', 'roberta-base', 'distilroberta-base'\n",
    "Model_path = 'roberta-base'\n",
    "Max_len = 300\n",
    "\n",
    "def bert_tokenize(X, model_path=Model_path, max_len=Max_len):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    tokenized = tokenizer(X.tolist(), padding='max_length',truncation=True, max_length=max_len, return_tensors=\"np\") \n",
    "    return {feat: tokenized[feat] for feat in tokenizer.model_input_names}\n",
    "\n",
    "def build_bert_model(model_path=Model_path, max_len=Max_len):\n",
    "\n",
    "    input_ids = tf.keras.Input(shape=(max_len, ),dtype='int32')\n",
    "    attention_mask = tf.keras.Input(shape=(max_len, ),dtype='int32')\n",
    "\n",
    "    bert_output = TFAutoModel.from_pretrained(model_path,\n",
    "                                              output_hidden_states=True, \n",
    "                                              hidden_dropout_prob=0.0,\n",
    "                                              layer_norm_eps=1e-7)(input_ids, attention_mask)\n",
    "\n",
    "    last_hidden_states = bert_output.hidden_states[-1]\n",
    "    # cls_token = bert_layer[:, 0, :]\n",
    "\n",
    "    # weight layer\n",
    "    weight_layer = Dense(512, activation='tanh')(last_hidden_states)\n",
    "    weight_layer = Dense(1, activation=softMaxAxis1)(weight_layer)\n",
    "\n",
    "    #  attention layer for weighted sum over the 300 token embedding vectors\n",
    "    layer = multiply([weight_layer,last_hidden_states])\n",
    "    layer = Lambda(lambda x: K.sum(x, axis=1))(layer)\n",
    "\n",
    "    # regression layer\n",
    "    out = Dense(1, activation='linear')(layer)\n",
    "    model = tf.keras.Model(inputs=[input_ids , attention_mask], outputs=out)\n",
    "    model.compile(optimizer=Adam(2e-5), loss = [rmse])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Training Fold 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x0000020A0E66F908>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x0000020A0E66F908>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py:5049: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n",
      "Epoch 1/15\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "454/454 [==============================] - ETA: 0s - loss: 0.6218WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "454/454 [==============================] - 134s 273ms/step - loss: 0.6218 - val_loss: 0.5143\n",
      "Epoch 2/15\n",
      "454/454 [==============================] - 122s 270ms/step - loss: 0.4312 - val_loss: 0.4727\n",
      "Epoch 3/15\n",
      "454/454 [==============================] - 121s 268ms/step - loss: 0.3430 - val_loss: 0.5237\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "Epoch 4/15\n",
      "454/454 [==============================] - 124s 273ms/step - loss: 0.2407 - val_loss: 0.4695\n",
      "Epoch 5/15\n",
      "454/454 [==============================] - 125s 276ms/step - loss: 0.1715 - val_loss: 0.4811\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-06.\n",
      "Epoch 6/15\n",
      "454/454 [==============================] - 124s 274ms/step - loss: 0.1236 - val_loss: 0.4625\n",
      "Epoch 7/15\n",
      "454/454 [==============================] - 123s 271ms/step - loss: 0.0888 - val_loss: 0.4569\n",
      "Epoch 8/15\n",
      "454/454 [==============================] - 120s 265ms/step - loss: 0.0781 - val_loss: 0.4569\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "Epoch 9/15\n",
      "454/454 [==============================] - 120s 264ms/step - loss: 0.0685 - val_loss: 0.4545\n",
      "Epoch 10/15\n",
      "454/454 [==============================] - 120s 265ms/step - loss: 0.0517 - val_loss: 0.4560\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-06.\n",
      "Epoch 11/15\n",
      "454/454 [==============================] - 120s 264ms/step - loss: 0.0454 - val_loss: 0.4565\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-07.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00011: early stopping\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "--------------------------------------------------\n",
      "Training Fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "Epoch 1/15\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "454/454 [==============================] - ETA: 0s - loss: 0.6051WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "454/454 [==============================] - 131s 269ms/step - loss: 0.6051 - val_loss: 0.5570\n",
      "Epoch 2/15\n",
      "454/454 [==============================] - 121s 266ms/step - loss: 0.4369 - val_loss: 0.5198\n",
      "Epoch 3/15\n",
      "454/454 [==============================] - 120s 265ms/step - loss: 0.3112 - val_loss: 0.4732\n",
      "Epoch 4/15\n",
      "454/454 [==============================] - 121s 265ms/step - loss: 0.2534 - val_loss: 0.4587\n",
      "Epoch 5/15\n",
      "454/454 [==============================] - 120s 265ms/step - loss: 0.2284 - val_loss: 0.4744\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "Epoch 6/15\n",
      "454/454 [==============================] - 120s 265ms/step - loss: 0.1716 - val_loss: 0.4612\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-06.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00006: early stopping\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "--------------------------------------------------\n",
      "Training Fold 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "Epoch 1/15\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "454/454 [==============================] - ETA: 0s - loss: 0.6274WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "454/454 [==============================] - 132s 269ms/step - loss: 0.6274 - val_loss: 0.5483\n",
      "Epoch 2/15\n",
      "454/454 [==============================] - 121s 266ms/step - loss: 0.4336 - val_loss: 0.5076\n",
      "Epoch 3/15\n",
      "454/454 [==============================] - 121s 266ms/step - loss: 0.3220 - val_loss: 0.4945\n",
      "Epoch 4/15\n",
      "454/454 [==============================] - 121s 266ms/step - loss: 0.2565 - val_loss: 0.4871\n",
      "Epoch 5/15\n",
      "454/454 [==============================] - 121s 266ms/step - loss: 0.2335 - val_loss: 0.4860\n",
      "Epoch 6/15\n",
      "454/454 [==============================] - 121s 266ms/step - loss: 0.2159 - val_loss: 0.4909\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "Epoch 7/15\n",
      "454/454 [==============================] - 121s 266ms/step - loss: 0.1656 - val_loss: 0.4786\n",
      "Epoch 8/15\n",
      "454/454 [==============================] - 123s 272ms/step - loss: 0.1100 - val_loss: 0.4772\n",
      "Epoch 9/15\n",
      "454/454 [==============================] - 122s 269ms/step - loss: 0.0975 - val_loss: 0.4724\n",
      "Epoch 10/15\n",
      "454/454 [==============================] - 122s 268ms/step - loss: 0.0979 - val_loss: 0.4799\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-06.\n",
      "Epoch 11/15\n",
      "454/454 [==============================] - 122s 268ms/step - loss: 0.0845 - val_loss: 0.4692\n",
      "Epoch 12/15\n",
      "454/454 [==============================] - 122s 268ms/step - loss: 0.0599 - val_loss: 0.4679\n",
      "Epoch 13/15\n",
      "454/454 [==============================] - 122s 268ms/step - loss: 0.0585 - val_loss: 0.4685\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "Epoch 14/15\n",
      "454/454 [==============================] - 122s 268ms/step - loss: 0.0509 - val_loss: 0.4677\n",
      "Epoch 15/15\n",
      "454/454 [==============================] - 122s 268ms/step - loss: 0.0392 - val_loss: 0.4690\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-06.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "--------------------------------------------------\n",
      "Training Fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "Epoch 1/15\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "454/454 [==============================] - ETA: 0s - loss: 0.6366WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "454/454 [==============================] - 132s 271ms/step - loss: 0.6366 - val_loss: 0.5136\n",
      "Epoch 2/15\n",
      "454/454 [==============================] - 121s 267ms/step - loss: 0.4361 - val_loss: 0.5188\n",
      "\n",
      "Epoch 00002: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "Epoch 3/15\n",
      "454/454 [==============================] - 121s 266ms/step - loss: 0.2963 - val_loss: 0.4777\n",
      "Epoch 4/15\n",
      "454/454 [==============================] - 122s 270ms/step - loss: 0.2052 - val_loss: 0.4769\n",
      "Epoch 5/15\n",
      "454/454 [==============================] - 121s 267ms/step - loss: 0.1703 - val_loss: 0.4680\n",
      "Epoch 6/15\n",
      "454/454 [==============================] - 121s 266ms/step - loss: 0.1456 - val_loss: 0.4833\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-06.\n",
      "Epoch 7/15\n",
      "454/454 [==============================] - 122s 269ms/step - loss: 0.1205 - val_loss: 0.4764\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00007: early stopping\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "--------------------------------------------------\n",
      "Training Fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "Epoch 1/15\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "454/454 [==============================] - ETA: 0s - loss: 0.6144WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "454/454 [==============================] - 133s 272ms/step - loss: 0.6144 - val_loss: 0.5296\n",
      "Epoch 2/15\n",
      "454/454 [==============================] - 121s 267ms/step - loss: 0.4425 - val_loss: 0.5192\n",
      "Epoch 3/15\n",
      "454/454 [==============================] - 121s 267ms/step - loss: 0.3305 - val_loss: 0.4899\n",
      "Epoch 4/15\n",
      "454/454 [==============================] - 121s 267ms/step - loss: 0.2602 - val_loss: 0.4719\n",
      "Epoch 5/15\n",
      "454/454 [==============================] - 121s 267ms/step - loss: 0.2306 - val_loss: 0.4631\n",
      "Epoch 6/15\n",
      "454/454 [==============================] - 121s 267ms/step - loss: 0.2020 - val_loss: 0.4900\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "Epoch 7/15\n",
      "454/454 [==============================] - 121s 267ms/step - loss: 0.1565 - val_loss: 0.4535\n",
      "Epoch 8/15\n",
      "454/454 [==============================] - 121s 267ms/step - loss: 0.1098 - val_loss: 0.4517\n",
      "Epoch 9/15\n",
      "454/454 [==============================] - 121s 267ms/step - loss: 0.1033 - val_loss: 0.4742\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-06.\n",
      "Epoch 10/15\n",
      "454/454 [==============================] - 121s 267ms/step - loss: 0.0858 - val_loss: 0.4574\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00010: early stopping\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "Our out of folds RMSE is 0.4845744988306818\n"
     ]
    }
   ],
   "source": [
    "# K-fold CV training\n",
    "kf = KFold(n_splits = 5 , shuffle = True , random_state = 36)\n",
    "cv_predictions = np.zeros(len(df))\n",
    "\n",
    "for fold , (train_index , val_index) in enumerate(kf.split(df)):\n",
    "    print('-'*50)\n",
    "    print(\"Training Fold {}\".format(fold))\n",
    "\n",
    "    X_train, y_train = df.iloc[train_index]['text'], df.iloc[train_index][['y']].values\n",
    "    X_val, y_val = df.iloc[val_index]['text'], df.iloc[val_index][['y']].values\n",
    "    \n",
    "    X_train_tokenized = bert_tokenize(X_train)\n",
    "    X_val_tokenized = bert_tokenize(X_val)\n",
    "    \n",
    "    model = build_bert_model(Model_path, Max_len)\n",
    "    save_path = \"model_fold_{}\".format(fold) +\".h5\"\n",
    "    checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "                                                    filepath=save_path, \n",
    "                                                    monitor='val_loss', \n",
    "                                                    save_best_only=True, \n",
    "                                                    mode='min', \n",
    "                                                    verbose=1)\n",
    "    \n",
    "    history = model.fit([X_train_tokenized['input_ids'],X_train_tokenized['attention_mask']], y_train, \n",
    "                     epochs=15, batch_size=5, \n",
    "                     validation_data = ([X_val_tokenized['input_ids'],X_val_tokenized['attention_mask']], y_val), \n",
    "                     callbacks=[reduce_lr, early_stop],\n",
    "                     verbose=1)\n",
    "\n",
    "    # Predict validation set to save them in the out of folds array\n",
    "    val_pred = model.predict([X_val_tokenized['input_ids'],X_val_tokenized['attention_mask']])\n",
    "    cv_predictions[val_index] = val_pred.reshape(-1)\n",
    "    \n",
    "    del model\n",
    "    gc.collect()\n",
    "    K.clear_session()\n",
    "\n",
    "# Calculate out of folds root mean squared error\n",
    "cv_rmse = np.sqrt(mean_squared_error(df['y'], cv_predictions))\n",
    "print(f'Our out of folds RMSE is {cv_rmse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "gc.collect()\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 x 2267 x 300\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': array([[    0,   133,   745, ...,     1,     1,     1],\n",
       "        [    0,   133,  1114, ...,     1,     1,     1],\n",
       "        [    0, 37818,     5, ...,     1,     1,     1],\n",
       "        ...,\n",
       "        [    0,   100,  1017, ...,     1,     1,     1],\n",
       "        [    0,  1121,   209, ...,     1,     1,     1],\n",
       "        [    0, 39488,    16, ...,     1,     1,     1]]),\n",
       " 'attention_mask': array([[1, 1, 1, ..., 0, 0, 0],\n",
       "        [1, 1, 1, ..., 0, 0, 0],\n",
       "        [1, 1, 1, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1, ..., 0, 0, 0],\n",
       "        [1, 1, 1, ..., 0, 0, 0],\n",
       "        [1, 1, 1, ..., 0, 0, 0]])}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# skipping k-fold training for quick model testing\n",
    "# test & train split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['text'], df[['y']].values, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train_tokenized = bert_tokenize(X_train)\n",
    "X_test_tokenized = bert_tokenize(X_test)\n",
    "\n",
    "print (\n",
    "str(len(X_train_tokenized))\\\n",
    "+ ' x ' +\\\n",
    "str(len(X_train_tokenized['input_ids']))\\\n",
    "+ ' x ' +\\\n",
    "str(len(X_train_tokenized['input_ids'][0]))\n",
    ")\n",
    "\n",
    "display(X_train_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x0000022B2F27F908>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x0000022B2F27F908>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py:5049: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n",
      "Epoch 1/15\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "295/295 [==============================] - ETA: 0s - loss: 0.6511WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "295/295 [==============================] - 98s 300ms/step - loss: 0.6511 - val_loss: 0.5751\n",
      "Epoch 2/15\n",
      "295/295 [==============================] - 98s 334ms/step - loss: 0.4656 - val_loss: 0.5168\n",
      "Epoch 3/15\n",
      "295/295 [==============================] - 89s 300ms/step - loss: 0.3455 - val_loss: 0.5207\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "Epoch 4/15\n",
      "295/295 [==============================] - 108s 365ms/step - loss: 0.2482 - val_loss: 0.4837\n",
      "Epoch 5/15\n",
      "295/295 [==============================] - 87s 294ms/step - loss: 0.1690 - val_loss: 0.4919\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-06.\n",
      "Epoch 6/15\n",
      "295/295 [==============================] - 88s 298ms/step - loss: 0.1262 - val_loss: 0.4769\n",
      "Epoch 7/15\n",
      "295/295 [==============================] - 137s 466ms/step - loss: 0.0915 - val_loss: 0.4816\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "Epoch 8/15\n",
      "295/295 [==============================] - 87s 295ms/step - loss: 0.0680 - val_loss: 0.4764\n",
      "Epoch 9/15\n",
      "295/295 [==============================] - 89s 300ms/step - loss: 0.0534 - val_loss: 0.4758\n",
      "Epoch 10/15\n",
      "295/295 [==============================] - 87s 296ms/step - loss: 0.0497 - val_loss: 0.4744\n",
      "Epoch 11/15\n",
      "295/295 [==============================] - 87s 296ms/step - loss: 0.0453 - val_loss: 0.4759\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-06.\n",
      "Epoch 12/15\n",
      "295/295 [==============================] - 91s 307ms/step - loss: 0.0437 - val_loss: 0.4780\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-07.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00012: early stopping\n",
      "18/18 [==============================] - 7s 383ms/step - loss: 0.4730\n",
      "Test Scores: 0.4730382263660431\n"
     ]
    }
   ],
   "source": [
    "model = build_bert_model(Model_path, Max_len)\n",
    "\n",
    "history = model.fit([X_train_tokenized['input_ids'],\\\n",
    "                     X_train_tokenized['attention_mask']],\\\n",
    "                     y_train, epochs=15, batch_size=5, validation_split=0.35, \n",
    "                     callbacks=[reduce_lr,early_stop],\n",
    "                     verbose=1)\n",
    "\n",
    "score = model.evaluate([X_test_tokenized['input_ids'],\\\n",
    "                        X_test_tokenized['attention_mask']],\\\n",
    "                        y_test, \n",
    "                        verbose=1)\n",
    "print(\"Test Scores:\", score)\n",
    "\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debugging\n",
    "\n",
    "intermediate_layer_model =tf.keras.Model(inputs=model.input,\n",
    "                                 outputs=model.get_layer('tf_roberta_model').output)\n",
    "intermediate_output = intermediate_layer_model.predict([X_test_tokenized['input_ids'][0:10],\\\n",
    "                        X_test_tokenized['attention_mask'][0:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intermediate_output[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intermediate_output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intermediate_output[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting training/validation loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################### Testing & Validation ################################\n",
    "BERTmodel = TFBertModel.from_pretrained('bert-base-uncased', output_hidden_states=True)\n",
    "a = tokenizer(df['text'].tolist()[0:5],padding=True,return_tensors='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERTmodel(a).last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERTmodel(a)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(BERTmodel(a).last_hidden_state))\n",
    "print(len(BERTmodel(a).last_hidden_state[0]))\n",
    "print(len(BERTmodel(a).last_hidden_state[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
